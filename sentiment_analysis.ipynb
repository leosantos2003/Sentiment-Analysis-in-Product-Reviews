{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068547de",
   "metadata": {},
   "source": [
    "## 1. Loading and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9f6ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          review_id                          order_id  \\\n",
      "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
      "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
      "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
      "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
      "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
      "\n",
      "   review_score review_comment_title  \\\n",
      "0             4                  NaN   \n",
      "1             5                  NaN   \n",
      "2             5                  NaN   \n",
      "3             5                  NaN   \n",
      "4             5                  NaN   \n",
      "\n",
      "                              review_comment_message review_creation_date  \\\n",
      "0                                                NaN  2018-01-18 00:00:00   \n",
      "1                                                NaN  2018-03-10 00:00:00   \n",
      "2                                                NaN  2018-02-17 00:00:00   \n",
      "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
      "4  Parabéns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
      "\n",
      "  review_answer_timestamp  \n",
      "0     2018-01-18 21:46:59  \n",
      "1     2018-03-11 03:05:13  \n",
      "2     2018-02-18 14:36:24  \n",
      "3     2017-04-21 22:02:06  \n",
      "4     2018-03-02 10:26:53  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99224 entries, 0 to 99223\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   review_id                99224 non-null  object\n",
      " 1   order_id                 99224 non-null  object\n",
      " 2   review_score             99224 non-null  int64 \n",
      " 3   review_comment_title     11568 non-null  object\n",
      " 4   review_comment_message   40977 non-null  object\n",
      " 5   review_creation_date     99224 non-null  object\n",
      " 6   review_answer_timestamp  99224 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 5.3+ MB\n",
      "None\n",
      "review_id                      0\n",
      "order_id                       0\n",
      "review_score                   0\n",
      "review_comment_title       87656\n",
      "review_comment_message     58247\n",
      "review_creation_date           0\n",
      "review_answer_timestamp        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading dataset\n",
    "file_path = 'olist_order_reviews_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Visualizing the first lines and information\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Checking null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd08a0d",
   "metadata": {},
   "source": [
    "## 2. Pre-processing and Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd0e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "                                                text  label\n",
      "0                                           n chegou      0\n",
      "1  Um dos relogios veio aberto sem tampa e sem fu...      0\n",
      "2  O produto chegou em 2 entregas, só que bem mal...      0\n",
      "3  Ao invés de vocês se preocuparem em resolver o...      0\n",
      "4  Estou aguardando entrega meu produto desde o d...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leors\\AppData\\Local\\Temp\\ipykernel_18120\\3624836757.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_reviews['sentiment'] = df_reviews['review_score'].apply(score_to_sentiment)\n",
      "C:\\Users\\leors\\AppData\\Local\\Temp\\ipykernel_18120\\3624836757.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_sampled = df_final.groupby('label').apply(lambda x: x.sample(n=sample_size, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Removing lines where the comment is null\n",
    "df_reviews = df.dropna(subset=['review_comment_message'])\n",
    "\n",
    "# Creating sentiment labels\n",
    "# Ratings 1 and 2 -> Negative (0)\n",
    "# Rating 3 -> Neutral (undefined for now)\n",
    "# Rating 4 and 5 (1)\n",
    "def score_to_sentiment(score):\n",
    "    if score in [1, 2]:\n",
    "        return 0 # Negative\n",
    "    elif score in [4, 5]:\n",
    "        return 1 # Positive\n",
    "    else:\n",
    "        return None # Neutral/undefined for now\n",
    "    \n",
    "df_reviews['sentiment'] = df_reviews['review_score'].apply(score_to_sentiment)\n",
    "\n",
    "# Removing the neutral reviews\n",
    "df_reviews = df_reviews.dropna(subset=['sentiment'])\n",
    "\n",
    "# Selecting only the reviews that interest\n",
    "df_final = df_reviews[['review_comment_message', 'sentiment']].copy()\n",
    "df_final.rename(columns={'review_comment_message': 'text', 'sentiment':'label'}, inplace=True)\n",
    "\n",
    "# Converting the 'label' column type to integer\n",
    "df_final['label'] = df_final['label'].astype(int)\n",
    "\n",
    "# The dataset is too big\n",
    "# We will take a more balanced dataset, for faster training\n",
    "sample_size = 5000 # 5000 positives and 5000 negatives\n",
    "df_sampled = df_final.groupby('label').apply(lambda x: x.sample(n=sample_size, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "print(df_sampled['label'].value_counts())\n",
    "print(df_sampled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9368b702",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a86632f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 22335.80 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Loading the dataser from the Pandas DataFrame\n",
    "dataset = Dataset.from_pandas(df_sampled)\n",
    "\n",
    "# Loading the BERT tokenizer in Portuguese\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to tokenize the texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Applying the tokenization to the whole dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Splitting into training and test (80% training, 20% test)\n",
    "final_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "\n",
    "print(final_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb794273",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning the BERT Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
