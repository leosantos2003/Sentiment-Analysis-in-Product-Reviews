{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253d0dcb",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6e99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          review_id                          order_id  \\\n",
      "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
      "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
      "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
      "3  e64fb393e7b32834bb789ff8bb30750e  658677c97b385a9be170737859d3511b   \n",
      "4  f7c4243c7fe1938f181bec41a392bdeb  8e6bfb81e283fa7e4f11123a3fb894f1   \n",
      "\n",
      "   review_score review_comment_title  \\\n",
      "0             4                  NaN   \n",
      "1             5                  NaN   \n",
      "2             5                  NaN   \n",
      "3             5                  NaN   \n",
      "4             5                  NaN   \n",
      "\n",
      "                              review_comment_message review_creation_date  \\\n",
      "0                                                NaN  2018-01-18 00:00:00   \n",
      "1                                                NaN  2018-03-10 00:00:00   \n",
      "2                                                NaN  2018-02-17 00:00:00   \n",
      "3              Recebi bem antes do prazo estipulado.  2017-04-21 00:00:00   \n",
      "4  Parabéns lojas lannister adorei comprar pela I...  2018-03-01 00:00:00   \n",
      "\n",
      "  review_answer_timestamp  \n",
      "0     2018-01-18 21:46:59  \n",
      "1     2018-03-11 03:05:13  \n",
      "2     2018-02-18 14:36:24  \n",
      "3     2017-04-21 22:02:06  \n",
      "4     2018-03-02 10:26:53  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99224 entries, 0 to 99223\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   review_id                99224 non-null  object\n",
      " 1   order_id                 99224 non-null  object\n",
      " 2   review_score             99224 non-null  int64 \n",
      " 3   review_comment_title     11568 non-null  object\n",
      " 4   review_comment_message   40977 non-null  object\n",
      " 5   review_creation_date     99224 non-null  object\n",
      " 6   review_answer_timestamp  99224 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 5.3+ MB\n",
      "None\n",
      "review_id                      0\n",
      "order_id                       0\n",
      "review_score                   0\n",
      "review_comment_title       87656\n",
      "review_comment_message     58247\n",
      "review_creation_date           0\n",
      "review_answer_timestamp        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "file_path = 'olist_order_reviews_dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Viewing the first lines and information\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Checking for null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5de08",
   "metadata": {},
   "source": [
    "## 2. Pre-processing and Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e167b1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    5000\n",
      "1    5000\n",
      "Name: count, dtype: int64\n",
      "                                                text  label\n",
      "0                                           n chegou      0\n",
      "1  Um dos relogios veio aberto sem tampa e sem fu...      0\n",
      "2  O produto chegou em 2 entregas, só que bem mal...      0\n",
      "3  Ao invés de vocês se preocuparem em resolver o...      0\n",
      "4  Estou aguardando entrega meu produto desde o d...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29726/2208477984.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_reviews['sentiment'] = df_reviews['review_score'].apply(score_to_sentiment)\n",
      "/tmp/ipykernel_29726/2208477984.py:38: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=sample_size, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# Removing lines where the comment is null\n",
    "df_reviews = df.dropna(subset=['review_comment_message'])\n",
    "\n",
    "\n",
    "# Creating the sentiment labels\n",
    "# Grades 1 and 2 -> Negative (0)\n",
    "# Grade 3 -> Neutral (let's ignore it for now to simplify the problem)\n",
    "# Grades 4 and 5 -> Positive (1)\n",
    "def score_to_sentiment(score):\n",
    "    if score in [1, 2]:\n",
    "        return 0  # Negative\n",
    "    elif score in [4, 5]:\n",
    "        return 1  # Positive\n",
    "    else:\n",
    "        return None  # Neutral/Undefined\n",
    "\n",
    "\n",
    "df_reviews['sentiment'] = df_reviews['review_score'].apply(score_to_sentiment)\n",
    "\n",
    "# Removing neutral reviews (grade 3)\n",
    "df_reviews = df_reviews.dropna(subset=['sentiment'])\n",
    "\n",
    "# Selecting only the columns that interest us\n",
    "df_final = df_reviews[['review_comment_message', 'sentiment']].copy()\n",
    "df_final.rename(\n",
    "    columns={'review_comment_message': 'text', 'sentiment': 'label'}, inplace=True\n",
    ")\n",
    "\n",
    "# Converting the type of the 'label' column to integer\n",
    "df_final['label'] = df_final['label'].astype(int)\n",
    "\n",
    "# The dataset is large\n",
    "# For a first test, let's take a balanced sample\n",
    "# This makes training much faster\n",
    "sample_size = 5000  # 5000 positives e 5000 negatives\n",
    "df_sampled = (\n",
    "    df_final.groupby('label')\n",
    "    .apply(lambda x: x.sample(n=sample_size, random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(df_sampled['label'].value_counts())\n",
    "print(df_sampled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e940943",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec7a3feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 32116.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Loading the dataset from the Pandas DataFrame\n",
    "dataset = Dataset.from_pandas(df_sampled)\n",
    "\n",
    "# Loading the BERT tokenizer in Portuguese\n",
    "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Function to tokenize texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "\n",
    "# Applying tokenization to the entire dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Splitting into training and testing (80% training, 20% testing)\n",
    "final_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "\n",
    "print(final_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb6fd0",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bd0e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.214400</td>\n",
       "      <td>0.192260</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.936903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.164900</td>\n",
       "      <td>0.206605</td>\n",
       "      <td>0.944500</td>\n",
       "      <td>0.944454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.085200</td>\n",
       "      <td>0.219412</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.947993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.18912522157033285, metrics={'train_runtime': 361.8196, 'train_samples_per_second': 66.331, 'train_steps_per_second': 4.146, 'total_flos': 681293188972800.0, 'train_loss': 0.18912522157033285, 'epoch': 3.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Loading the pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "# Function to calculate metrics during training\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "\n",
    "# Creating the Data Collator, which will take care of the padding in each batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Defining the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Directory to save the model\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=16,  # Training batch size\n",
    "    per_device_eval_batch_size=16,  # Evaluation batch size\n",
    "    warmup_steps=500,  # Optimizer warm-up steps\n",
    "    weight_decay=0.01,  # Weight decay\n",
    "    logging_dir='./logs',  # Directory for logs\n",
    "    logging_steps=100,\n",
    "    eval_strategy='epoch',  # Evaluate at the end of each season\n",
    "    save_strategy='epoch',  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end\n",
    ")\n",
    "\n",
    "# Creating the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_datasets['train'],\n",
    "    eval_dataset=final_datasets['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Begin training (this may take a while)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf78713",
   "metadata": {},
   "source": [
    "## 5. Evaluating and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bd305b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados da Avaliação Final:\n",
      "{'eval_loss': 0.19226007163524628, 'eval_accuracy': 0.937, 'eval_f1': 0.9369027269598926, 'eval_runtime': 6.3185, 'eval_samples_per_second': 316.531, 'eval_steps_per_second': 19.783, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: 'Adorei o produto, chegou muito rápido e é de ótima qualidade!'\n",
      "Resultado: [{'label': 'LABEL_1', 'score': 0.9961045980453491}]\n",
      "Texto: 'Péssima experiência, o produto veio quebrado e a entrega demorou semanas.'\n",
      "Resultado: [{'label': 'LABEL_0', 'score': 0.9972532391548157}]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the final model on the test set\n",
    "evaluation_results = trainer.evaluate()\n",
    "print('Resultados da Avaliação Final:')\n",
    "print(evaluation_results)\n",
    "\n",
    "# To make a prediction in a new text\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a text classification pipeline with the trained model\n",
    "# trainer.save_model() saves the model to './results/checkpoint-...'\n",
    "# Choose the checkpoint with the best result\n",
    "best_model_path = './results/checkpoint-1500'\n",
    "sentiment_analyzer = pipeline(\n",
    "    'sentiment-analysis', model=best_model_path, tokenizer=model_name\n",
    ")\n",
    "\n",
    "# Usage example\n",
    "texto_positivo = 'Adorei o produto, chegou muito rápido e é de ótima qualidade!'\n",
    "resultado_pos = sentiment_analyzer(texto_positivo)\n",
    "print(f\"Texto: '{texto_positivo}'\\nResultado: {resultado_pos}\")\n",
    "\n",
    "texto_negativo = (\n",
    "    'Péssima experiência, o produto veio quebrado e a entrega demorou semanas.'\n",
    ")\n",
    "resultado_neg = sentiment_analyzer(texto_negativo)\n",
    "print(f\"Texto: '{texto_negativo}'\\nResultado: {resultado_neg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
